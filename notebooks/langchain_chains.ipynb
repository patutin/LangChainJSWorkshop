{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain interface**\n",
    "```javascript\n",
    "import { CallbackManagerForChainRun } from \"langchain/callbacks\";\n",
    "import { BaseMemory } from \"langchain/memory\";\n",
    "import { ChainValues } from \"langchain/schema\";\n",
    "\n",
    "abstract class BaseChain {\n",
    "  memory?: BaseMemory;\n",
    "\n",
    "  /**\n",
    "   * Run the core logic of this chain and return the output\n",
    "   */\n",
    "  abstract _call(\n",
    "    values: ChainValues,\n",
    "    runManager?: CallbackManagerForChainRun\n",
    "  ): Promise<ChainValues>;\n",
    "\n",
    "  /**\n",
    "   * Return the string type key uniquely identifying this class of chain.\n",
    "   */\n",
    "  abstract _chainType(): string;\n",
    "\n",
    "  /**\n",
    "   * Return the list of input keys this chain expects to receive when called.\n",
    "   */\n",
    "  abstract get inputKeys(): string[];\n",
    "\n",
    "  /**\n",
    "   * Return the list of output keys this chain will produce when called.\n",
    "   */\n",
    "  abstract get outputKeys(): string[];\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { OpenAI } from \"npm:langchain/llms/openai\";\n",
    "import { PromptTemplate } from \"npm:langchain/prompts\";\n",
    "import { LLMChain } from \"npm:langchain/chains\";\n",
    "\n",
    "// We can construct an LLMChain from a PromptTemplate and an LLM.\n",
    "const model = new OpenAI({ temperature: 0 });\n",
    "const prompt = PromptTemplate.fromTemplate(\n",
    "  \"What is a good name for a company that makes {product}?\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = new LLMChain({ llm: model, prompt });\n",
    "\n",
    "// Since this LLMChain is a single-input, single-output chain, we can also `run` it.\n",
    "// This convenience method takes in a string and returns the value\n",
    "// of the output key field in the chain response. For LLMChains, this defaults to \"text\".\n",
    "const res = await chain.run(\"colorful socks\");\n",
    "console.log( res );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ```SimpleSequentialChain``` is a chain that allows you to join multiple single-input/single-output chains into one chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank you for your feedback! It sounds like you really enjoyed the Pizza Salami. We're so glad to hear that the crust was crisp, the salami was flavorful and juicy, the cheese was melted to perfection, and the tomato sauce was just the right amount of tangy. We appreciate your recommendation and we hope you enjoy your next meal with us!\n"
     ]
    }
   ],
   "source": [
    "import { SimpleSequentialChain, LLMChain } from \"npm:langchain/chains\";\n",
    "\n",
    "const criticTemplate = `\n",
    "You are a restaurant critic. You've tasted {dish} and are expressing your impression of it. \n",
    "The review can be either negative or positive.\n",
    "`;\n",
    "\n",
    "const assistantTemplate = `\n",
    "You are an assistant bot. Your job is to make the customer feel heard and understood.\n",
    "Reflect on the input you receive.\n",
    "\n",
    "text: {review}\n",
    "`;\n",
    "\n",
    "const criticPromptTemplate = new PromptTemplate({\n",
    "  template: criticTemplate,\n",
    "  inputVariables: [\"dish\"],\n",
    "});\n",
    "\n",
    "const assistantPromptTemplate = new PromptTemplate({\n",
    "  template: assistantTemplate,\n",
    "  inputVariables: [\"review\"],\n",
    "});\n",
    "\n",
    "\n",
    "const reviewChain1 = new LLMChain({ llm: model, prompt: criticPromptTemplate });\n",
    "const reviewChain2 = new LLMChain({ llm: model, prompt: assistantPromptTemplate });\n",
    "\n",
    "const overallChain = new SimpleSequentialChain({\n",
    "  chains: [reviewChain1, reviewChain2],\n",
    "  verbose: false,\n",
    "});\n",
    "\n",
    "const result = await overallChain.run(\"Pizza Salami\");\n",
    "\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language\n",
    "\n",
    "# Interface\n",
    "\n",
    "In an effort to make it as easy as possible to create custom chains, we've implemented a [\"Runnable\"](/docs/api/schema_runnable/classes/Runnable) protocol that most components implement.\n",
    "This is a standard interface with a few different methods, which make it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:\n",
    "\n",
    "- `stream`: stream back chunks of the response\n",
    "- `invoke`: call the chain on an input\n",
    "- `batch`: call the chain on a list of inputs\n",
    "\n",
    "The **input type** varies by component :\n",
    "\n",
    "| Component      | Input Type                                          |\n",
    "| -------------- | --------------------------------------------------- |\n",
    "| Prompt         | Object                                              |\n",
    "| Retriever      | Single string                                       |\n",
    "| LLM, ChatModel | Single string, list of chat messages or PromptValue |\n",
    "| Tool           | Single string, or object, depending on the tool     |\n",
    "| OutputParser   | The output of an LLM or ChatModel                   |\n",
    "\n",
    "The **output type** also varies by component :\n",
    "\n",
    "| Component    | Output Type           |\n",
    "| ------------ | --------------------- |\n",
    "| LLM          | String                |\n",
    "| ChatModel    | ChatMessage           |\n",
    "| Prompt       | PromptValue           |\n",
    "| Retriever    | List of documents     |\n",
    "| Tool         | Depends on the tool   |\n",
    "| OutputParser | Depends on the parser |\n",
    "\n",
    "You can combine runnables (and runnable-like objects such as functions and objects whose values are all functions) into sequences in two ways:\n",
    "\n",
    "- Call the `.pipe` instance method, which takes another runnable-like as an argument\n",
    "- Use the `RunnableSequence.from([])` static method with an array of runnable-likes, which will run in sequence when invoked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"npm:langchain/chat_models/openai\";\n",
    "\n",
    "const model = new ChatOpenAI({});\n",
    "const promptTemplate = PromptTemplate.fromTemplate(\n",
    "  \"You are a restaurant critic. Express your impression about {dish} in 10 words\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = promptTemplate.pipe(model);\n",
    "\n",
    "const stream = await chain.stream({ dish: \"Pizza Salami\" });\n",
    "\n",
    "for await (const chunk of stream) {\n",
    "  console.log(chunk?.content);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { RunnableSequence } from \"npm:langchain/schema/runnable\";\n",
    "\n",
    "// You can also create a chain using an array of runnables\n",
    "const chain = RunnableSequence.from([promptTemplate, model]);\n",
    "\n",
    "const result = await chain.invoke({ dish: \"Pizza Salami\" });\n",
    "\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const chain = promptTemplate.pipe(model);\n",
    "\n",
    "const result = await chain.batch([{ dish: \"Pizza\" }, { dish: \"Sushi\" }]);\n",
    "\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to LLMChain expamle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { PromptTemplate } from \"npm:langchain/prompts\";\n",
    "import { StringOutputParser } from \"npm:langchain/schema/output_parser\";\n",
    "import { RunnableSequence } from \"npm:langchain/schema/runnable\";\n",
    "\n",
    "const criticPrompt = PromptTemplate.fromTemplate(\n",
    "    `\n",
    "    You are a restaurant critic. You've tasted {dish} and are expressing your impression of it. \n",
    "    The review can be either negative or positive.\n",
    "    `\n",
    ");\n",
    "\n",
    "const assistantPrompt = PromptTemplate.fromTemplate(\n",
    "  `\n",
    "  You are an assistant bot. Your job is to make the customer feel heard and understood.\n",
    "  Reflect on the input you receive.\n",
    "  \n",
    "  text: {review}\n",
    "  `\n",
    ");\n",
    "\n",
    "const model = new ChatOpenAI({});\n",
    "\n",
    "const criticChain = criticPrompt.pipe(model).pipe(new StringOutputParser());\n",
    "\n",
    "const combinedChain = RunnableSequence.from([\n",
    "  {\n",
    "    review: criticChain,\n",
    "  },\n",
    "  assistantPrompt,\n",
    "  model,\n",
    "  new StringOutputParser(),\n",
    "]);\n",
    "\n",
    "const result = await combinedChain.invoke({\n",
    "  dish: \"Pizza\"\n",
    "});\n",
    "\n",
    "console.log(result);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-model chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { ChatOpenAI } from \"npm:langchain/chat_models/openai\";\n",
    "import { PromptTemplate } from \"npm:langchain/prompts\";\n",
    "import { StringOutputParser } from \"npm:langchain/schema/output_parser\";\n",
    "import { RunnableSequence } from \"npm:langchain/schema/runnable\";\n",
    "import { HfInference } from 'npm:@huggingface/inference';\n",
    "\n",
    "const criticPrompt = PromptTemplate.fromTemplate(\n",
    "    `\n",
    "    You are a restaurant critic. You've tasted {dish} and are expressing your impression of it. \n",
    "    `\n",
    ");\n",
    "\n",
    "const assistantPrompt = PromptTemplate.fromTemplate(\n",
    "  `\n",
    "  Generate image of the {dishdescription}.\n",
    "  `\n",
    ");\n",
    "\n",
    "const model = new ChatOpenAI({});\n",
    "const hfi = new HfInference();\n",
    "\n",
    "const criticChain = criticPrompt.pipe(model).pipe(new StringOutputParser());\n",
    "\n",
    "const combinedChain = RunnableSequence.from([\n",
    "  criticChain,\n",
    "  (imagePrompt: string) => hfi.textToImage({\n",
    "      model: 'stabilityai/stable-diffusion-2',\n",
    "      inputs: imagePrompt,\n",
    "      parameters: { negative_prompt: 'blurry' },\n",
    "  })\n",
    "]);\n",
    "\n",
    "const imageBlob = await combinedChain.invoke({\n",
    "  dish: \"Pizza\"\n",
    "});\n",
    "\n",
    "const imageBuf = await imageBlob.arrayBuffer();\n",
    "\n",
    "const filePath = './test.jpg';\n",
    "const bytes = new Uint8Array(imageBuf);\n",
    "await Deno.writeFile(filePath, bytes);\n",
    "\n",
    "console.log(\"![Generated Image](./test.jpg)\");\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nb_converter": "script",
   "pygments_lexer": "typescript",
   "version": "5.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
